{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb49d74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For Hugging Face datasets + transformers\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17487494",
   "metadata": {},
   "source": [
    "### Load the same splits from TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0fc871d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>word_count</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FANTASTIC! TRUMP BUDGET DIRECTOR Rips Into Rep...</td>\n",
       "      <td>WOW! THIS IS FANTASTIC! WE HIGHLY RECOMMEND TH...</td>\n",
       "      <td>politics</td>\n",
       "      <td>May 24, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>FANTASTIC! TRUMP BUDGET DIRECTOR Rips Into Rep...</td>\n",
       "      <td>119</td>\n",
       "      <td>fantastic! trump budget director rips into rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump Just Got Ripped To SHREDS After Blaming...</td>\n",
       "      <td>As you know, Donald Trump put aside his disdai...</td>\n",
       "      <td>News</td>\n",
       "      <td>January 7, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>Trump Just Got Ripped To SHREDS After Blaming...</td>\n",
       "      <td>1150</td>\n",
       "      <td>trump just got ripped to shreds after blaming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GLOBAL CLIMATE CHANGE LIARS Ignore Truth About...</td>\n",
       "      <td>Global Climate cooling warming change frauds a...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>Apr 22, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>GLOBAL CLIMATE CHANGE LIARS Ignore Truth About...</td>\n",
       "      <td>878</td>\n",
       "      <td>global climate change liars ignore truth about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WILL VILE LEFTISTS Turn Democrats Away?…WATCH ...</td>\n",
       "      <td>A group of mostly pro-life college students ca...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Feb 24, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>WILL VILE LEFTISTS Turn Democrats Away?…WATCH ...</td>\n",
       "      <td>287</td>\n",
       "      <td>will vile leftists turn democrats away? watch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No-confidence vote against Pennsylvania approv...</td>\n",
       "      <td>HARRISBURG, Pa. (Reuters) - The city council o...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>January 20, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>No-confidence vote against Pennsylvania approv...</td>\n",
       "      <td>406</td>\n",
       "      <td>no confidence vote against pennsylvania approv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  FANTASTIC! TRUMP BUDGET DIRECTOR Rips Into Rep...   \n",
       "1   Trump Just Got Ripped To SHREDS After Blaming...   \n",
       "2  GLOBAL CLIMATE CHANGE LIARS Ignore Truth About...   \n",
       "3  WILL VILE LEFTISTS Turn Democrats Away?…WATCH ...   \n",
       "4  No-confidence vote against Pennsylvania approv...   \n",
       "\n",
       "                                                text          subject  \\\n",
       "0  WOW! THIS IS FANTASTIC! WE HIGHLY RECOMMEND TH...         politics   \n",
       "1  As you know, Donald Trump put aside his disdai...             News   \n",
       "2  Global Climate cooling warming change frauds a...  Government News   \n",
       "3  A group of mostly pro-life college students ca...        left-news   \n",
       "4  HARRISBURG, Pa. (Reuters) - The city council o...     politicsNews   \n",
       "\n",
       "                date  label  \\\n",
       "0       May 24, 2017      0   \n",
       "1    January 7, 2017      0   \n",
       "2       Apr 22, 2016      0   \n",
       "3       Feb 24, 2017      0   \n",
       "4  January 20, 2016       1   \n",
       "\n",
       "                                             content  word_count  \\\n",
       "0  FANTASTIC! TRUMP BUDGET DIRECTOR Rips Into Rep...         119   \n",
       "1   Trump Just Got Ripped To SHREDS After Blaming...        1150   \n",
       "2  GLOBAL CLIMATE CHANGE LIARS Ignore Truth About...         878   \n",
       "3  WILL VILE LEFTISTS Turn Democrats Away?…WATCH ...         287   \n",
       "4  No-confidence vote against Pennsylvania approv...         406   \n",
       "\n",
       "                                       clean_content  \n",
       "0  fantastic! trump budget director rips into rep...  \n",
       "1  trump just got ripped to shreds after blaming ...  \n",
       "2  global climate change liars ignore truth about...  \n",
       "3  will vile leftists turn democrats away? watch ...  \n",
       "4  no confidence vote against pennsylvania approv...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "val_df = pd.read_csv(\"../data/val.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Ensure no NaNs in text or labels\n",
    "for df in (train_df, val_df, test_df):\n",
    "    df[\"content\"] = df[\"content\"].fillna(\"\")\n",
    "    df[\"label\"] = df[\"label\"].fillna(-1).astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cfff4b",
   "metadata": {},
   "source": [
    "### Convert to Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36466d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['content', 'label'],\n",
       "     num_rows: 31428\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['content', 'label'],\n",
       "     num_rows: 6735\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['content', 'label'],\n",
       "     num_rows: 6735\n",
       " }))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only the needed columns\n",
    "train_ds = Dataset.from_pandas(train_df[[\"content\", \"label\"]], preserve_index=False)\n",
    "val_ds = Dataset.from_pandas(val_df[[\"content\", \"label\"]], preserve_index=False)\n",
    "test_ds = Dataset.from_pandas(test_df[[\"content\", \"label\"]], preserve_index=False)\n",
    "\n",
    "train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945d19a",
   "metadata": {},
   "source": [
    "### Load tokenizer & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f04e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26b28c",
   "metadata": {},
   "source": [
    "### Tokenization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4430a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 31428/31428 [00:09<00:00, 3435.21 examples/s]\n",
      "Map: 100%|██████████| 6735/6735 [00:02<00:00, 3029.60 examples/s]\n",
      "Map: 100%|██████████| 6735/6735 [00:02<00:00, 3010.29 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'input_ids': [101,\n",
       "  10392,\n",
       "  999,\n",
       "  8398,\n",
       "  5166,\n",
       "  2472,\n",
       "  10973,\n",
       "  2015,\n",
       "  2046,\n",
       "  6398,\n",
       "  11242,\n",
       "  19044,\n",
       "  7659,\n",
       "  1024,\n",
       "  1523,\n",
       "  2057,\n",
       "  1521,\n",
       "  2128,\n",
       "  2025,\n",
       "  2183,\n",
       "  2000,\n",
       "  2079,\n",
       "  2070,\n",
       "  1997,\n",
       "  1996,\n",
       "  4689,\n",
       "  2477,\n",
       "  1996,\n",
       "  2627,\n",
       "  3447,\n",
       "  2106,\n",
       "  1524,\n",
       "  1031,\n",
       "  2678,\n",
       "  1033,\n",
       "  10166,\n",
       "  999,\n",
       "  2023,\n",
       "  2003,\n",
       "  10392,\n",
       "  999,\n",
       "  2057,\n",
       "  3811,\n",
       "  16755,\n",
       "  1996,\n",
       "  2972,\n",
       "  2678,\n",
       "  1024,\n",
       "  2436,\n",
       "  1997,\n",
       "  2968,\n",
       "  1998,\n",
       "  26178,\n",
       "  1006,\n",
       "  18168,\n",
       "  2497,\n",
       "  1007,\n",
       "  2472,\n",
       "  10872,\n",
       "  14163,\n",
       "  22144,\n",
       "  5420,\n",
       "  22106,\n",
       "  1996,\n",
       "  3252,\n",
       "  1010,\n",
       "  7848,\n",
       "  2015,\n",
       "  1998,\n",
       "  5682,\n",
       "  1997,\n",
       "  1996,\n",
       "  8398,\n",
       "  3447,\n",
       "  10807,\n",
       "  2095,\n",
       "  2760,\n",
       "  5166,\n",
       "  1996,\n",
       "  26457,\n",
       "  5166,\n",
       "  1996,\n",
       "  2190,\n",
       "  2112,\n",
       "  1997,\n",
       "  2023,\n",
       "  2307,\n",
       "  2739,\n",
       "  3034,\n",
       "  2003,\n",
       "  2043,\n",
       "  1037,\n",
       "  6398,\n",
       "  5176,\n",
       "  2055,\n",
       "  7659,\n",
       "  2000,\n",
       "  4785,\n",
       "  2671,\n",
       "  3454,\n",
       "  1024,\n",
       "  2012,\n",
       "  1996,\n",
       "  2459,\n",
       "  1024,\n",
       "  4002,\n",
       "  2928,\n",
       "  10872,\n",
       "  14163,\n",
       "  22144,\n",
       "  5420,\n",
       "  10973,\n",
       "  2015,\n",
       "  2046,\n",
       "  1996,\n",
       "  6398,\n",
       "  1998,\n",
       "  2009,\n",
       "  1055,\n",
       "  2074,\n",
       "  12476,\n",
       "  999,\n",
       "  2057,\n",
       "  16755,\n",
       "  1996,\n",
       "  2972,\n",
       "  2678,\n",
       "  2138,\n",
       "  2017,\n",
       "  2222,\n",
       "  2156,\n",
       "  8398,\n",
       "  5086,\n",
       "  2028,\n",
       "  6047,\n",
       "  17387,\n",
       "  999,\n",
       "  10872,\n",
       "  14163,\n",
       "  22144,\n",
       "  5420,\n",
       "  4282,\n",
       "  1996,\n",
       "  5166,\n",
       "  1998,\n",
       "  16024,\n",
       "  1996,\n",
       "  2811,\n",
       "  17950,\n",
       "  999,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 512  # BERT max sequence length\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"content\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "train_tokenized = train_ds.map(tokenize_function, batched=True)\n",
    "val_tokenized = val_ds.map(tokenize_function, batched=True)\n",
    "test_tokenized = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the original text column to keep things clean\n",
    "train_tokenized = train_tokenized.remove_columns([\"content\"])\n",
    "val_tokenized = val_tokenized.remove_columns([\"content\"])\n",
    "test_tokenized = test_tokenized.remove_columns([\"content\"])\n",
    "\n",
    "train_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449e346",
   "metadata": {},
   "source": [
    "### Metrics function for Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "997a060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d570ae",
   "metadata": {},
   "source": [
    "### TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a89a885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/distilbert_fake_news\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10484e9e",
   "metadata": {},
   "source": [
    "### Create the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2208a125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adam1\\AppData\\Local\\Temp\\ipykernel_21336\\3669286204.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca0ec8",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dbaf9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 31,428\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11,787\n",
      "  Number of trainable parameters = 66,955,010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11787' max='11787' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11787/11787 1:18:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002771</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.999689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.999844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.999844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6735\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../models/distilbert_fake_news\\checkpoint-3929\n",
      "Configuration saved in ../models/distilbert_fake_news\\checkpoint-3929\\config.json\n",
      "Model weights saved in ../models/distilbert_fake_news\\checkpoint-3929\\model.safetensors\n",
      "tokenizer config file saved in ../models/distilbert_fake_news\\checkpoint-3929\\tokenizer_config.json\n",
      "Special tokens file saved in ../models/distilbert_fake_news\\checkpoint-3929\\special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6735\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../models/distilbert_fake_news\\checkpoint-7858\n",
      "Configuration saved in ../models/distilbert_fake_news\\checkpoint-7858\\config.json\n",
      "Model weights saved in ../models/distilbert_fake_news\\checkpoint-7858\\model.safetensors\n",
      "tokenizer config file saved in ../models/distilbert_fake_news\\checkpoint-7858\\tokenizer_config.json\n",
      "Special tokens file saved in ../models/distilbert_fake_news\\checkpoint-7858\\special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6735\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../models/distilbert_fake_news\\checkpoint-11787\n",
      "Configuration saved in ../models/distilbert_fake_news\\checkpoint-11787\\config.json\n",
      "Model weights saved in ../models/distilbert_fake_news\\checkpoint-11787\\model.safetensors\n",
      "tokenizer config file saved in ../models/distilbert_fake_news\\checkpoint-11787\\tokenizer_config.json\n",
      "Special tokens file saved in ../models/distilbert_fake_news\\checkpoint-11787\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../models/distilbert_fake_news\\checkpoint-7858 (score: 0.9998515219005196).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11787, training_loss=0.004434759405527627, metrics={'train_runtime': 4736.5967, 'train_samples_per_second': 19.905, 'train_steps_per_second': 2.488, 'total_flos': 1.2489556214882304e+16, 'train_loss': 0.004434759405527627, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "train_result = trainer.train()\n",
    "train_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05f65b",
   "metadata": {},
   "source": [
    "### Evaluate on validation & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fb49e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6735\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1684' max='842' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [842/842 03:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0011605075560510159,\n",
       " 'eval_accuracy': 0.9998515219005196,\n",
       " 'eval_f1': 0.9998444064104559,\n",
       " 'eval_runtime': 76.0759,\n",
       " 'eval_samples_per_second': 88.53,\n",
       " 'eval_steps_per_second': 11.068,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Validation metrics:\")\n",
    "val_metrics = trainer.evaluate(eval_dataset=val_tokenized)\n",
    "val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71885695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6735\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.001472154282964766,\n",
       " 'eval_accuracy': 0.9997030438010394,\n",
       " 'eval_f1': 0.9996886674968867,\n",
       " 'eval_runtime': 78.3792,\n",
       " 'eval_samples_per_second': 85.928,\n",
       " 'eval_steps_per_second': 10.743,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test metrics:\")\n",
    "test_metrics = trainer.evaluate(eval_dataset=test_tokenized)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936067bb",
   "metadata": {},
   "source": [
    "### Save model & tokenizer for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "537ba59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../models/distilbert_fake_news\n",
      "Configuration saved in ../models/distilbert_fake_news\\config.json\n",
      "Model weights saved in ../models/distilbert_fake_news\\model.safetensors\n",
      "tokenizer config file saved in ../models/distilbert_fake_news\\tokenizer_config.json\n",
      "Special tokens file saved in ../models/distilbert_fake_news\\special_tokens_map.json\n",
      "tokenizer config file saved in ../models/distilbert_fake_news\\tokenizer_config.json\n",
      "Special tokens file saved in ../models/distilbert_fake_news\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DistilBERT model and tokenizer to: ../models/distilbert_fake_news\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"../models/distilbert_fake_news\"\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"Saved DistilBERT model and tokenizer to:\", save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
